import feedparser
import requests
import os
import time

def download_arxiv_papers(query="bioinformatics", max_results=5, save_dir="arxiv_pdfs"):
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)

    # æ„é€ æœç´¢ URLï¼ˆ åŸºäº RSSï¼‰
    feed_url = f"http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}"

    print(f" æ­£åœ¨æœç´¢å…³é”®è¯ï¼š{query}ï¼ˆå…± {max_results} ç¯‡ï¼‰")
    feed = feedparser.parse(feed_url)

    for i, entry in enumerate(feed.entries):
        title = entry.title.strip().replace('\n', ' ')
        print(f"\n[{i+1}] ğŸ“„ {title}")

        # PDF ä¸‹è½½é“¾æ¥
        pdf_url = entry.id.replace('abs', 'pdf') + ".pdf"

        try:
            # æ„é€ ä¿å­˜è·¯å¾„
            safe_title = ''.join(c for c in title if c.isalnum() or c in " _-")[:50]
            filename = os.path.join(save_dir, f"{safe_title}.pdf")

            # ä¸‹è½½ PDF
            response = requests.get(pdf_url)
            if response.status_code == 200:
                with open(filename, 'wb') as f:
                    f.write(response.content)
                print(f" ä¸‹è½½æˆåŠŸï¼š{filename}")
            else:
                print(f" ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")

        except Exception as e:
            print(f" å‡ºé”™ï¼š{e}")

        time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

    print(f" å·²å®Œæˆä¸‹è½½ï¼Œæ–‡ä»¶ä¿å­˜åœ¨æ–‡ä»¶å¤¹ï¼š{save_dir}/")

# æ”¹queryé‡Œçš„å…³é”®è¯å°±å¥½
download_arxiv_papers(query="bioinformatics", max_results=10)
